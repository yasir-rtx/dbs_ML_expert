{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Retrieve dataset from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'indonlu' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/indobenchmark/indonlu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    " \n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "from indonlu.utils.forward_fn import forward_sequence_classification\n",
    "from indonlu.utils.metrics import document_sentiment_metrics_fn\n",
    "from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    " \n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed(19072021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Setup Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
    " \n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124443651"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_param(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Setup Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = \"/home/yelf/Desktop/dbs_ML_expert/#02-SentimentAnalysis/indonlu/dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv\"\n",
    "valid_dataset_path = \"/home/yelf/Desktop/dbs_ML_expert/#02-SentimentAnalysis/indonlu/dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv\"\n",
    "test_dataset_path = \"/home/yelf/Desktop/dbs_ML_expert/#02-SentimentAnalysis/indonlu/dataset/smsa_doc-sentiment-prosa/test_preprocess.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Define Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kode berikut hanya untuk menunjukan bagaimana kelas DocumenSentiment Dataset dan Loader bekerja / Tidak perlu ditulis\n",
    "# class DocumentSentimentDataset(Dataset):\n",
    "#     # Static constant variable\n",
    "#     LABEL2INDEX = {'positive': 0, 'neutral': 1, 'negative': 2} # Map dari label string ke index\n",
    "#     INDEX2LABEL = {0: 'positive', 1: 'neutral', 2: 'negative'} # Map dari Index ke label string\n",
    "#     NUM_LABELS = 3 # Jumlah label\n",
    "   \n",
    "#     def load_dataset(self, path):\n",
    "#         df = pd.read_csv(path, sep='\\t', header=None) # Baca tsv file dengan pandas\n",
    "#         df.columns = ['text','sentiment'] # Berikan nama pada kolom tabel\n",
    "#         df['sentiment'] = df['sentiment'].apply(lambda lab: self.LABEL2INDEX[lab]) # Konversi string label ke index\n",
    "#         return df\n",
    "   \n",
    "#     def __init__(self, dataset_path, tokenizer, *args, **kwargs):\n",
    "#         self.data = self.load_dataset(dataset_path) # Load tsv file\n",
    " \n",
    "#         # Assign tokenizer, disini kita menggunakan tokenizer subword dari HuggingFace\n",
    "#         self.tokenizer = tokenizer \n",
    " \n",
    "#     def __getitem__(self, index):\n",
    "#         data = self.data.loc[index,:] # Ambil data pada baris tertentu dari tabel\n",
    "#         text, sentiment = data['text'], data['sentiment'] # Ambil nilai text dan sentiment\n",
    "#         subwords = self.tokenizer.encode(text) # Tokenisasi text menjadi subword\n",
    "    \n",
    "#         # Return numpy array dari subwords dan label\n",
    "#         return np.array(subwords), np.array(sentiment), data['text']\n",
    "   \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)  # Return panjang dari dataset\n",
    "\n",
    "# class DocumentSentimentDataLoader(DataLoader):\n",
    "#     def __init__(self, max_seq_len=512, *args, **kwargs):\n",
    "#         super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n",
    "#         self.max_seq_len = max_seq_len # Assign batas maksimum subword\n",
    "#         self.collate_fn = self._collate_fn # Assign fungsi collate_fn dengan fungsi yang kita definisikan\n",
    "       \n",
    "#     def _collate_fn(self, batch):\n",
    "#         batch_size = len(batch) # Ambil batch size\n",
    "#         max_seq_len = max(map(lambda x: len(x[0]), batch)) # Cari panjang subword maksimal dari batch \n",
    "#         max_seq_len = min(self.max_seq_len, max_seq_len) # Bandingkan dengan batas yang kita tentukan sebelumnya\n",
    "       \n",
    "#         # Buat buffer untuk subword, mask, dan sentiment labels, inisialisasikan semuanya dengan 0\n",
    "#         subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "#         mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "#         sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n",
    "       \n",
    "#         # Isi semua buffer\n",
    "#         for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n",
    "#             subwords = subwords[:max_seq_len]\n",
    "#             subword_batch[i,:len(subwords)] = subwords\n",
    "#             mask_batch[i,:len(subwords)] = 1\n",
    "#             sentiment_batch[i,0] = sentiment\n",
    "           \n",
    "#         # Return subword, mask, dan sentiment data\n",
    "#         return subword_batch, mask_batch, sentiment_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
    "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    " \n",
    "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=12, shuffle=True)  \n",
    "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=12, shuffle=False)  \n",
    "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    2,  6540,    92,  2970,   213,  4259,  3553,   899,    34,\n",
      "         259,  5590,   262,  2558,   386,   899,  1687,    26,  1574,\n",
      "       30470,   899,  3310, 30468, 22130, 30360,  6123,  6368, 30468,\n",
      "       22130, 30360,  2652,  1746, 30468,  8869,  6540,    34,  6315,\n",
      "        1622,  1256,  8949,   899, 30468,  4222,  1622,   752,   245,\n",
      "         295,  2083, 30470,  2346,  7107,   300, 30470,   405,   724,\n",
      "        5189, 30470,   843, 17464,   899,   540, 10989,  3331,  1107,\n",
      "       30468,   119,  3221,    79,    34,  2170,    98,  9167, 30457,\n",
      "           3]), array(0), 'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !')\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to Index : {'positive': 0, 'neutral': 1, 'negative': 2}\n",
      "Index to Word : {0: 'positive', 1: 'neutral', 2: 'negative'}\n"
     ]
    }
   ],
   "source": [
    "w2i = DocumentSentimentDataset.LABEL2INDEX  # word to index\n",
    "i2w = DocumentSentimentDataset.INDEX2LABEL  # index to word\n",
    "print(\"Word to Index :\", w2i)\n",
    "print(\"Index to Word :\", i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Uji Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text     : Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita\n",
      "Token    : [2, 4771, 10413, 722, 3300, 3466, 19227, 457, 34, 2176, 17377, 155, 3]\n",
      "Token(2) : tensor([[    2,  4771, 10413,   722,  3300,  3466, 19227,   457,    34,  2176,\n",
      "         17377,   155,     3]])\n",
      "logits   : tensor([[ 0.5018, -0.2294, -0.3558]], grad_fn=<AddmmBackward0>)\n",
      "Label    : 0\n"
     ]
    }
   ],
   "source": [
    "text = 'Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita'\n",
    "print(\"Text     :\", text)\n",
    "\n",
    "subwords = tokenizer.encode(text)\n",
    "print(\"Token    :\", subwords)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)  # convert to torch tensor\n",
    "print(\"Token(2) :\", subwords)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "print(\"logits   :\", logits)\n",
    "\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "print(\"Label    :\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Bahagia hatiku melihat pernikahan putri sulungku yang cantik jelita | Label : positive (52.480%)\n"
     ]
    }
   ],
   "source": [
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine Tuning dan Evaluasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimezier to Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-6) # learning rate = 0.000003\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    " \n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    " \n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    " \n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    " \n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    " \n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    " \n",
    "    # Calculate train metric\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    " \n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    " \n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        \n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    " \n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    " \n",
    "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        \n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics_to_string(metrics)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
